---
title: "Computationally Eï¬\x83cient Suï¬\x83cient Dimension Reduction via Squared-Loss
  Mutual Information"
abstract: 'The purpose of sufficient dimension reduction (SDR) is to find a low-dimensional
  expression of input features that is sufficient for predicting output values. In
  this paper, we propose a novel distribution-free SDR method called sufficient component
  analysis (SCA), which is computationally more efficient than existing methods. In
  our method, a solution is computed by iteratively performing dependence estimation
  and maximization: Dependence estimation is analytically carried out by recently-proposed
  least-squares mutual information (LSMI), and dependence maximization is also analytically
  carried out by utilizing the Epanechnikov kernel. Through large-scale experiments
  on real-world image classification and audio tagging problems, the proposed method
  is shown to compare favorably with existing dimension reduction approaches.'
pdf: http://proceedings.mlr.press/v20/yamada11/yamada11.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yamada11
month: 0
firstpage: 247
lastpage: 262
page: 247-262
sections: 
author:
- given: M.
  family: Yamada
- given: G.
  family: Niu
- given: J.
  family: Takagi
- given: M.
  family: Sugiyama
date: 2011-11-17
address: South Garden Hotels and Resorts, Taoyuan, Taiwain
publisher: PMLR
container-title: Proceedings of the Asian Conference on Machine Learning
volume: '20'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 11
  - 17
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
