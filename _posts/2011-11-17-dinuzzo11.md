---
title: Learning low-rank output kernels
abstract: Output kernel learning techniques allow to simultaneously learn a vector-valued
  function and a positive semidefinite matrix which describes the relationships between
  the outputs. In this paper, we introduce a new formulation that imposes a low-rank
  constraint on the output kernel and operates directly on a factor of the kernel
  matrix. First, we investigate the connection between output kernel learning and
  a regularization problem for an architecture with two layers. Then, we show that
  a variety of methods such as nuclear norm regularized regression, reduced-rank regression,
  principal component analysis, and low rank matrix approximation can be seen as special
  cases of the output kernel learning framework. Finally, we introduce a block coordinate
  descent strategy for learning low-rank output kernels.
pdf: http://proceedings.pmlr.press/dinuzzo11/dinuzzo11.pdf
layout: inproceedings
id: dinuzzo11
month: 0
firstpage: 181
lastpage: 196
page: 181-196
origpdf: http://jmlr.org/proceedings/papers/v20/dinuzzo11/dinuzzo11.pdf
sections: 
author:
- given: F.
  family: Dinuzzo
- given: K.
  family: Fukumizu
date: 2011-11-17
publisher: PMLR
container-title: Proceedings of the Asian Conference on Machine Learning
volume: '20'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 11
  - 17
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
