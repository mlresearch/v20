---
title: "Improving Policy Gradient Estimates with Inï¬\x82uence Information"
abstract: In reinforcement learning (RL) it is often possible to obtain sound, but
  incomplete, information about influences and independencies among problem variables
  and rewards, even when an exact domain model is unknown. For example, such information
  can be computed based on a partial, qualitative domain model, or via domain-specific
  analysis techniques. While, intuitively, such information appears useful for RL,
  there are no algorithms that incorporate it in a sound way. In this work, we describe
  how to leverage such information for improving the estimation of policy gradients,
  which can be used to speedup gradient-based RL. We prove general conditions under
  which our estimator is unbiased and show that it will typically have reduced variance
  compared to standard unbiased gradient estimates. We evaluate the approach in the
  domain of Adaptation-Based Programming where RL is used to optimize the performance
  of programs and independence information can be computed via standard program analysis
  techniques. Incorporating independence information produces a large speedup in learning
  on a variety of adaptive programs.
pdf: "./pinto11/pinto11.pdf"
layout: inproceedings
id: pinto11
month: 0
firstpage: 1
lastpage: 18
page: 1-18
origpdf: http://jmlr.org/proceedings/papers/v20/pinto11/pinto11.pdf
sections: 
author:
- given: J.
  family: Pinto
- given: A.
  family: Fern
- given: T.
  family: Bauer
- given: M.
  family: Erwig
date: '2011-11-17 00:00:01'
publisher: PMLR
---
